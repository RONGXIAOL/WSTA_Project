{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90042 Project 2019: Automatic Fact Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: build index for doc content query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_path = '../'\n",
    "wiki_pages_path = root_path + 'materials/wiki-pages-text/'\n",
    "pages_list = os.listdir(wiki_pages_path)\n",
    "pages_list.sort()\n",
    "\n",
    "doc_line_indexes = []  # a list of dictionary that saves doc's line id\n",
    "## [{'1986_NBA_Finals': 1, \n",
    "##   '1789_Dobrovolsky': 227,\n",
    "##   '1596_in_Scotland': 618, ...}, \n",
    "##   {...}, ...]\n",
    "\n",
    "head_pages_index = {}  # first 4 charaters of doc appears in which pages\n",
    "## {'1986':['../materials/wiki-pages-text/wiki-001.txt', '../materials/wiki-pages-text/wiki-002.txt', '../materials/wiki-pages-text/wiki-003.txt'],\n",
    "##  '1789':['../materials/wiki-pages-text/wiki-001.txt', '../materials/wiki-pages-text/wiki-002.txt'],\n",
    "##  '1596':['../materials/wiki-pages-text/wiki-001.txt', '../materials/wiki-pages-text/wiki-002.txt']}\n",
    "\n",
    "for page_name in pages_list:  # 'wiki-001.txt'\n",
    "    page_path = wiki_pages_path + page_name  # '../materials/wiki-pages-text/wiki-001.txt'\n",
    "    with open(page_path,'r',encoding='utf-8') as f:\n",
    "        doc_line_index = {}  # a dictionary that saves docs' line id :\n",
    "                             ##  {'1986_NBA_Finals': 1, \n",
    "                             ##   '1789_Dobrovolsky': 227,\n",
    "                             ##   '1596_in_Scotland': 618, ...}\n",
    "        pre = ''\n",
    "        lines = f.readlines()\n",
    "        for line_id, line in enumerate(lines):\n",
    "            tokens = line.split()  # ['1986_NBA_Finals', '2', 'The', 'Celtics', 'defeated', 'the', 'Rockets', ...]\n",
    "            doc_name = tokens[0]  # '1986_NBA_Finals'\n",
    "            if doc_name != pre:  # if there is a new doc that appears first time\n",
    "                doc_line_index[doc_name] = line_id  # record its line id\n",
    "                head = doc_name[:4]  # '1986_NBA_Finals' => '1986'\n",
    "                if head not in head_pages_index:\n",
    "                    head_pages_index[head] = [page_path]\n",
    "                elif page_path not in head_pages_index[head]:\n",
    "                    head_pages_index[head].append(page_path)\n",
    "                pre = doc_name\n",
    "        doc_line_indexes.append(doc_line_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head_pages_index['Bawb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: get content and save as a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used:  131.25512504577637\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "## input : (\"Party_of_Hellenism\", 3)\n",
    "def get_content(doc_name, sent_ids):\n",
    "    contents = []\n",
    "    sent_ids = sent_ids\n",
    "    head = unicodedata.normalize('NFC', doc_name)[:4]\n",
    "    evidence_pages = head_pages_index[head]\n",
    "    for evidence_page in evidence_pages:\n",
    "        wiki_index = int(evidence_page[-7:-4]) - 1\n",
    "        if doc_name in doc_line_indexes[wiki_index]:\n",
    "            with open(evidence_page,'r',encoding='utf-8') as f:\n",
    "                lines = f.readlines()[doc_line_indexes[wiki_index][doc_name]:]\n",
    "                for line in lines:\n",
    "                    if line.startswith(doc_name):\n",
    "                        for sent_id in sent_ids:\n",
    "                            if line.startswith(doc_name+\" \"+str(sent_id)+\" \"):\n",
    "                                contents.append(line)\n",
    "                                doc, _id = line.split()[:2]\n",
    "                                if doc != doc_name or _id != str(sent_id):\n",
    "                                    print(wiki_index+1, doc,doc_name,_id,sent_id)\n",
    "    return contents\n",
    "\n",
    "## read train set into a dictionary\n",
    "train_path = root_path + 'materials/160train.json'\n",
    "save_path = root_path + 'materials/160train_pro.json'\n",
    "new_dic = {}\n",
    "with open(train_path,'r',encoding='utf-8') as f:\n",
    "    train_dic = json.load(f)\n",
    "    for _id in train_dic:\n",
    "        claim = unicodedata.normalize('NFC', train_dic[_id]['claim'])\n",
    "        label = train_dic[_id]['label']\n",
    "        evidence_contents = []\n",
    "        doc_sentids = {}\n",
    "        for evid in train_dic[_id]['evidence']:\n",
    "            if evid[0] in doc_sentids:\n",
    "                doc_sentids[evid[0]].append(evid[1])\n",
    "            else:\n",
    "                doc_sentids[evid[0]] = [evid[1]]\n",
    "        for doc_name in doc_sentids:\n",
    "            try:\n",
    "                evidence_contents.extend(get_content(unicodedata.normalize('NFC', doc_name), doc_sentids[doc_name]))\n",
    "            except Exception as e:\n",
    "                print(e, doc_name, doc_sentids[doc_name])\n",
    "        new_dic[_id] = {'claim':claim,'evidence':evidence_contents,'label':label}\n",
    "                \n",
    "with open(save_path,'w',encoding='utf-8') as f:\n",
    "    json.dump(new_dic,f,indent=4)\n",
    "            \n",
    "print('time used: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build entity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from allennlp.predictors.predictor import Predictor\n",
    "# predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('/Users/liurongxiao/pythonAPI/stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "                           '/Users/liurongxiao/pythonAPI/stanford-ner-2018-10-16/stanford-ner.jar',\n",
    "                           encoding='utf-8')\n",
    "\n",
    "import unicodedata\n",
    "import json\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "root_path = '../'\n",
    "entities = []\n",
    "train_path = root_path + 'materials/160train.json'\n",
    "with open(train_path,'r',encoding='utf-8') as f:\n",
    "    dic = json.load(f)\n",
    "    num = 0\n",
    "    for _id in dic:\n",
    "        if num > 10:\n",
    "            break\n",
    "        num += 1\n",
    "        \n",
    "        claim = unicodedata.normalize('NFC', dic[_id]['claim'])\n",
    "#         pred = predictor.predict(sentence=claim)\n",
    "        \n",
    "        try:\n",
    "            classified_text = st.tag(claim)\n",
    "#             sentence = ' '.join([line[2:] for line in lines])\n",
    "    #         pred = predictor.predict(sentence=sentence[:1000000])\n",
    "#             print(1)\n",
    "    #         tags = pred['tags']\n",
    "#             print(2)\n",
    "    #         words = pred['words']\n",
    "            for text in classified_text:\n",
    "                if text[1] != 'O':\n",
    "                    if text[0] not in entities:\n",
    "                        entities.append(text[0])\n",
    "            print(num)\n",
    "        except Exception as e:\n",
    "            print(e, line)\n",
    "print('time used: ', time.time() - start)\n",
    "#     all_entities.append(entities)\n",
    "# with open('./entities.json','w',encoding='utf-8') as f:\n",
    "# #     json.dump(entities,f,indent=4)\n",
    "#     f.write(entities)\n",
    "\n",
    "# print('time used: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare the processed train with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "with open(\"../materials/train_pro.json\",'r',encoding='utf-8') as f:\n",
    "    dic1 = json.load(f)\n",
    "with open(\"../materials/train.json\",'r',encoding='utf-8') as f:\n",
    "    dic2 = json.load(f)\n",
    "for _id in dic1:\n",
    "    list1 = sorted([' '.join( sent.split()[:2]) for sent in dic1[_id][\"evidence\"]])\n",
    "    list2 = sorted([unicodedata.normalize('NFC',evid[0])+\" \"+str(evid[1]) for evid in dic2[_id][\"evidence\"]])\n",
    "    if list1!=list2:\n",
    "        print(list1)\n",
    "        print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_name in pages_list:  # 'wiki-001.txt'\n",
    "    page_path = wiki_pages_path + page_name  # '../materials/wiki-pages-text/wiki-001.txt'\n",
    "    with open(page_path,'r',encoding='utf-8') as f:\n",
    "        f_content = f.read()\n",
    "        if f_content.find(\"Adrienne Bailon\")!=-1:\n",
    "            print(f_content.find(\"Adrienne Bailon\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*token_embedder_tokens\\._projection.*weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "137334 SUPPORTS REFUTES\n",
      "111897 REFUTES SUPPORTS\n",
      "54168 REFUTES SUPPORTS\n",
      "204443 REFUTES SUPPORTS\n",
      "192714 SUPPORTS REFUTES\n",
      "107786 SUPPORTS REFUTES\n",
      "197381 REFUTES SUPPORTS\n",
      "142454 REFUTES SUPPORTS\n",
      "104386 REFUTES SUPPORTS\n",
      "128123 REFUTES SUPPORTS\n",
      "41665 REFUTES SUPPORTS\n",
      "21775 REFUTES SUPPORTS\n",
      "66638 REFUTES SUPPORTS\n",
      "227130 REFUTES SUPPORTS\n",
      "114567 REFUTES SUPPORTS\n",
      "163980 REFUTES SUPPORTS\n",
      "34412 REFUTES SUPPORTS\n",
      "79538 REFUTES SUPPORTS\n",
      "172478 REFUTES SUPPORTS\n",
      "60977 REFUTES SUPPORTS\n",
      "32820 REFUTES SUPPORTS\n",
      "186996 SUPPORTS REFUTES\n",
      "68084 SUPPORTS REFUTES\n",
      "4713 REFUTES SUPPORTS\n",
      "time used:  81.8645761013031\n",
      "76 100\n",
      "Precision: 0.76\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")\n",
    "\n",
    "pronoun = [\"he\", \"she\", \"they\", \"it\"]\n",
    "possessive = [\"his\", \"her\", \"their\", \"its\"]\n",
    "start = time.time()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with open(\"../materials/devset_pro_NFC.json\", 'r', encoding='utf-8') as f:\n",
    "    dic = json.load(f)\n",
    "    print(len(dic))\n",
    "    for _id in dic:\n",
    "        judges = Counter()\n",
    "        label = dic[_id][\"label\"]\n",
    "        claim = dic[_id][\"claim\"]\n",
    "        evidences = dic[_id][\"evidence\"]\n",
    "        sents = []\n",
    "        for line in evidences:\n",
    "            line = re.sub(r'-LRB-', '(', line)\n",
    "            line = re.sub(r'-RRB-', ')', line)\n",
    "            line = re.sub(r'-LSB-', '[', line)\n",
    "            line = re.sub(r'-RSB-', ']', line)\n",
    "            tokens = line.strip(\"\\n \").split()\n",
    "            topic = re.sub(\"\\(.*?\\)\", \"\",\" \".join(tokens[0].split(\"_\")))\n",
    "\n",
    "            evidence = tokens[2:]\n",
    "            for i in range(len(evidence)):\n",
    "                if evidence[i].lower() in pronoun:\n",
    "                    evidence[i] = topic\n",
    "                elif evidence[i].lower() in possessive:\n",
    "                    evidence[i] = topic + \"'s\"\n",
    "            evidence = \" \".join(evidence)\n",
    "            sents.append(evidence)\n",
    "        premise = \" \".join(sents)\n",
    "        if not evidences:\n",
    "            final_judge = \"NOT ENOUGH INFO\"\n",
    "        else:\n",
    "            pred = predictor.predict(premise=premise, hypothesis=claim)\n",
    "            probs = pred[\"label_probs\"]\n",
    "            judge = probs.index(max(probs))\n",
    "            if judge == 0:\n",
    "                final_judge = \"SUPPORTS\"\n",
    "            else:\n",
    "                final_judge = \"REFUTES\"\n",
    "\n",
    "        if final_judge == label:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(_id, label, final_judge)\n",
    "        total += 1\n",
    "        if total == 100:\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "print('time used: ', time.time() - start)\n",
    "print(correct,total)\n",
    "print(\"Precision:\", str(correct/total))\n",
    "\n",
    "# s0 = 'It is a GIRL, his (phone)'\n",
    "# s1 = re.sub(r'\\(.*?\\)', '', s0)\n",
    "# print(s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
